{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9zOopyQd0xRM8xB8jIZO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohandaz/HIA-Research-Project/blob/main/TB_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vFtTzq5FWor"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Jul  7 23:28:34 2024\n",
        "\n",
        "@author: Mohan\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.layers import Flatten, Dropout, Dense, Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications import VGG16\n",
        "import tensorflow as tf\n",
        "from skimage.exposure import equalize_adapthist\n",
        "from skimage.filters import gaussian\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Database: CSV file and image folder\n",
        "csv_path = '/Volumes/MHIA/Mix/mix_data.csv'\n",
        "image_dir = \"/Volumes/MHIA/Mix/xray\"\n",
        "\n",
        "metadata_df = pd.read_csv(csv_path)\n",
        "metadata_df.reset_index(inplace=True)\n",
        "print(\"Variables:\")\n",
        "print(metadata_df.columns)\n",
        "\n",
        "print(\"\\nNull entries check:\")\n",
        "print(metadata_df.isnull().sum())\n",
        "\n",
        "# Add .png extension to study_id (Note: not required if study_id includes file extension (.png))\n",
        "metadata_df[\"study_id\"] = metadata_df[\"study_id\"].astype(str) + \".png\"\n",
        "\n",
        "# Create labels\n",
        "metadata_df[\"labels\"] = metadata_df[\"findings\"].copy()\n",
        "metadata_df[\"labels\"] = metadata_df[\"labels\"].str.lower()\n",
        "\n",
        "# Get all image arrays\n",
        "img_arrs = {name_: None for name_ in metadata_df[\"study_id\"]}\n",
        "corrupted_files = []\n",
        "for name_ in tqdm(img_arrs.keys()):\n",
        "    if name_.startswith(\"._\"):\n",
        "        corrupted_files.append(name_)\n",
        "        continue\n",
        "    try:\n",
        "        image = load_img(\n",
        "            os.path.join(image_dir, name_),\n",
        "            target_size=(224, 224), color_mode=\"grayscale\"\n",
        "        )\n",
        "        image_arr = img_to_array(image, data_format=\"channels_last\", dtype=None)\n",
        "        img_arrs[name_] = image_arr\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {name_}: {e}\")\n",
        "        corrupted_files.append(name_)\n",
        "\n",
        "# Remove corrupted files\n",
        "metadata_df = metadata_df[~metadata_df[\"study_id\"].isin(corrupted_files)]\n",
        "\n",
        "# Preprocess images\n",
        "img_arrs_preprocessed = {name_: {} for name_ in img_arrs.keys() if img_arrs[name_] is not None}\n",
        "for name_, arr_ in tqdm(img_arrs.items()):\n",
        "    if arr_ is None:\n",
        "        continue\n",
        "\n",
        "    # Original\n",
        "    img_arrs_preprocessed[name_]['original'] = arr_\n",
        "\n",
        "    # Normalized\n",
        "    arr_normalized = arr_ / arr_.max()\n",
        "    img_arrs_preprocessed[name_]['normalized'] = arr_normalized\n",
        "\n",
        "    # Histogram equalization\n",
        "    arr_clahe = equalize_adapthist(arr_normalized, kernel_size=15, clip_limit=0.05)\n",
        "    img_arrs_preprocessed[name_]['equalized'] = arr_clahe\n",
        "\n",
        "    # Gaussian smoothing\n",
        "    arr_gaussian = gaussian(arr_clahe, sigma=0.5)\n",
        "    img_arrs_preprocessed[name_]['smoothed'] = arr_gaussian\n",
        "\n",
        "    # Scaling\n",
        "    arr_scaled = tf.image.per_image_standardization(arr_gaussian)\n",
        "    img_arrs_preprocessed[name_]['scaled'] = arr_scaled.numpy()\n",
        "\n",
        "# Prepare the final dataset\n",
        "images = np.array([img_arrs_preprocessed[name_]['scaled'] for name_ in img_arrs_preprocessed.keys()])\n",
        "\n",
        "# Convert 'findings' to binary labels: 'tb' -> 1, 'normal' -> 0\n",
        "labels = metadata_df[\"findings\"].apply(lambda x: 1 if x.lower() == 'tb' else 0).values\n",
        "\n",
        "# Ensure all images have the correct shape\n",
        "print(\"Shapes of preprocessed images:\")\n",
        "for img in images[:5]:\n",
        "    print(img.shape)\n",
        "\n",
        "# Convert grayscale images to 3 channels for VGG16\n",
        "def convert_to_rgb(x):\n",
        "    return np.repeat(x, 3, axis=-1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=None)\n",
        "\n",
        "X_train_rgb = np.stack([convert_to_rgb(img) for img in X_train])\n",
        "X_test_rgb = np.stack([convert_to_rgb(img) for img in X_test])\n",
        "\n",
        "print(f\"Shape of X_train_rgb: {X_train_rgb.shape}\")\n",
        "print(f\"Shape of X_test_rgb: {X_test_rgb.shape}\")\n",
        "\n",
        "# Verify the shape of one sample image\n",
        "print(f\"Shape of one sample image: {X_train_rgb[0].shape}\")\n",
        "\n",
        "# Define the VGG model\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=input_shape\n",
        ")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=input_shape))  # Explicitly define input shape\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.00009), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Ensure model summary works\n",
        "model.summary()\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "train_data_gen = datagen.flow(X_train_rgb, y_train, batch_size=32)\n",
        "\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    validation_data=(X_test_rgb, y_test),\n",
        "    epochs=60,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "# Plot training history\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
        "ax[0].plot(history.history['loss'], 'r', label='train')\n",
        "ax[0].plot(history.history['val_loss'], 'b', label='val')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(history.history['accuracy'], 'r', label='train')\n",
        "ax[1].plot(history.history['val_accuracy'], 'b', label='val')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test data set\n",
        "results = model.evaluate(X_test_rgb, y_test, batch_size=64)\n",
        "print(\"Test loss, Test acc:\", results)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = model.predict(X_test_rgb)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "# Visuzlisation\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Normal', 'TB'], yticklabels=['Normal', 'TB'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Class-wise performance metrics\n",
        "print(classification_report(y_test, y_pred_classes, target_names=['Normal', 'TB']))\n",
        "\n",
        "# ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate metrics\n",
        "def calculate_metrics(y_true, y_pred, y_pred_prob):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    auroc = roc_auc_score(y_true, y_pred_prob)\n",
        "    sensitivity = recall_score(y_true, y_pred)\n",
        "    specificity = recall_score(y_true, y_pred, pos_label=0)\n",
        "    ppv = precision_score(y_true, y_pred)\n",
        "    npv = precision_score(y_true, y_pred, pos_label=0)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    return acc, auroc, sensitivity, specificity, ppv, npv, f1\n",
        "\n",
        "# Predictions on training set\n",
        "y_train_pred = model.predict(X_train_rgb)\n",
        "y_train_pred_classes = (y_train_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate metrics for training set\n",
        "train_acc, train_auroc, train_sensitivity, train_specificity, train_ppv, train_npv, train_f1 = calculate_metrics(y_train, y_train_pred_classes, y_train_pred)\n",
        "\n",
        "# Calculate metrics for testing set\n",
        "test_acc, test_auroc, test_sensitivity, test_specificity, test_ppv, test_npv, test_f1 = calculate_metrics(y_test, y_pred_classes, y_pred)\n",
        "\n",
        "# Prepare the data for the table\n",
        "data = {\n",
        "    'Metric': ['AUROC', 'Accuracy', 'Sensitivity', 'Specificity', 'PPV', 'NPV', 'F1'],\n",
        "    'Training': [train_auroc, train_acc, train_sensitivity, train_specificity, train_ppv, train_npv, train_f1],\n",
        "    'Testing': [test_auroc, test_acc, test_sensitivity, test_specificity, test_ppv, test_npv, test_f1]\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "metrics_df = pd.DataFrame(data)\n",
        "\n",
        "# Display the table\n",
        "print(metrics_df)\n",
        "\n",
        "\n",
        "# Save the model to the specified path in .h5 format\n",
        "save_path = '/Volumes/MHIA/Mix/mix_model_VGG3.h5'\n",
        "model.save(save_path, save_format='h5')\n",
        "print(f\"Model saved successfully to {save_path}.\")\n",
        "\n",
        "\n",
        "# Save the model in the recommended Keras format\n",
        "save_path_keras = '/Volumes/MHIA/Mix/mix_model_VGG3.keras'\n",
        "model.save(save_path_keras)\n",
        "print(f\"Model saved successfully to {save_path_keras}.\")\n",
        "\n",
        "\n",
        "################ to print sample image ###########\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "# Select 3 sample images\n",
        "sample_images = list(img_arrs_preprocessed.keys())[:3]\n",
        "print(f\"Selected sample images: {sample_images}\")\n",
        "\n",
        "# Define a function to plot preprocessing steps\n",
        "def plot_preprocessing_steps(samples):\n",
        "    fig, axes = plt.subplots(nrows=len(samples), ncols=5, figsize=(20, 12))\n",
        "    steps = ['original', 'normalized', 'equalized', 'smoothed', 'scaled']\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        for j, step in enumerate(steps):\n",
        "            axes[i, j].imshow(img_arrs_preprocessed[sample][step].reshape(224, 224), cmap='gray')\n",
        "            axes[i, j].set_title(step.capitalize(), fontsize=10)\n",
        "            axes[i, j].axis('off')\n",
        "            axes[i, j].set_xlabel(f'{sample}', fontsize=8)\n",
        "            axes[i, j].set_ylabel(f'Image {i+1}', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the preprocessing steps\n",
        "plot_preprocessing_steps(sample_images)\n",
        "\n",
        "# Define the data generator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Function to plot augmentation steps\n",
        "def plot_augmentation_steps(samples):\n",
        "    fig, axes = plt.subplots(nrows=len(samples), ncols=6, figsize=(24, 12))\n",
        "    augmentation_labels = ['Original', 'Rotation', 'Width Shift', 'Height Shift', 'Zoom', 'Horizontal Flip']\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        original_image = np.expand_dims(img_arrs_preprocessed[sample]['scaled'], axis=-1)\n",
        "        axes[i, 0].imshow(original_image.reshape(224, 224), cmap='gray')\n",
        "        axes[i, 0].set_title('Original', fontsize=10)\n",
        "        axes[i, 0].axis('off')\n",
        "        axes[i, 0].set_xlabel(f'{sample}', fontsize=8)\n",
        "        axes[i, 0].set_ylabel(f'Image {i+1}', fontsize=8)\n",
        "\n",
        "        for j in range(1, 6):\n",
        "            aug_image = datagen.flow(np.expand_dims(img_arrs_preprocessed[sample]['scaled'], axis=0), batch_size=1).__next__()[0]\n",
        "            axes[i, j].imshow(aug_image.reshape(224, 224), cmap='gray')\n",
        "            axes[i, j].set_title(augmentation_labels[j], fontsize=10)\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the augmentation steps\n",
        "plot_augmentation_steps(sample_images)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}